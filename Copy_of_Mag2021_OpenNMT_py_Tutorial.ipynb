{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Mag2021-OpenNMT-py_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnvsl/Projects/blob/main/Copy_of_Mag2021_OpenNMT_py_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlamFUsz7WZ4"
      },
      "source": [
        "# **Инструкция OpenNMT-py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrL95kgH0-d2"
      },
      "source": [
        "Автор: Елена Шукшина"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9GO0UKm7j6d"
      },
      "source": [
        "**OpenNMT-py** – это открытая библиотека для нейронного машинного перевода, использующая PyTorch для реализации алгоритмов машинного обучения. Она проста в использовании и не требует глубоких знаний в области нейронных сетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS0L3iOtouiX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE6z3c592nf6"
      },
      "source": [
        "В этом блокноте мы познакомимся с оснвными принциами работы библиотеки OpenNMT-py. В качестве примера мы обучим модель французско-английского перевода на небольшом параллельном корпусе (137 860 редложений) с очень маленьким словарем (всего 357 слов на французском и 231 слово на английском). \n",
        "\n",
        "\n",
        "**Перед началом удостоверьтесь, что в *Изменить > Настройки блокнота > Аппаратный ускоритель* выбран GPU.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoLNjEkq9KbR"
      },
      "source": [
        "## **Шаг 1. Установка**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRJAxY8V-7I7"
      },
      "source": [
        "Скачать репозиторий OpenNMT-py версии 1.2.0 (запустите код, нажав на стрелочку в квадратных скобках ниже)\n",
        "\n",
        "Первая строчка скачивает репозиторий, вторая – меняет версию на нужную. \n",
        "\n",
        "После выполнения ячейки в меню с папочкой слева можно будет посмотреть содержимое репозитория."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1xZmzPL9mWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4969f3f-a358-4b4f-8f10-6db22feab6ed"
      },
      "source": [
        "!git clone https://github.com/OpenNMT/OpenNMT-py\n",
        "!cd OpenNMT-py && git checkout 60125c8"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenNMT-py'...\n",
            "remote: Enumerating objects: 17644, done.\u001b[K\n",
            "remote: Counting objects: 100% (603/603), done.\u001b[K\n",
            "remote: Compressing objects: 100% (298/298), done.\u001b[K\n",
            "remote: Total 17644 (delta 360), reused 485 (delta 295), pack-reused 17041\u001b[K\n",
            "Receiving objects: 100% (17644/17644), 273.78 MiB | 28.41 MiB/s, done.\n",
            "Resolving deltas: 100% (12668/12668), done.\n",
            "Note: checking out '60125c8'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 60125c80 Bump v1.2.0 (#1850)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEwecDut-0GL"
      },
      "source": [
        "Установить OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cow8c99hvZj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd759fc7-5dc8-48cf-9934-b6dea7d2c685"
      },
      "source": [
        "!python OpenNMT-py/setup.py install"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating OpenNMT_py.egg-info\n",
            "writing OpenNMT_py.egg-info/PKG-INFO\n",
            "writing dependency_links to OpenNMT_py.egg-info/dependency_links.txt\n",
            "writing entry points to OpenNMT_py.egg-info/entry_points.txt\n",
            "writing requirements to OpenNMT_py.egg-info/requires.txt\n",
            "writing top-level names to OpenNMT_py.egg-info/top_level.txt\n",
            "writing manifest file 'OpenNMT_py.egg-info/SOURCES.txt'\n",
            "reading manifest file 'OpenNMT_py.egg-info/SOURCES.txt'\n",
            "writing manifest file 'OpenNMT_py.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/OpenNMT_py-1.2.0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing OpenNMT_py-1.2.0-py3.7.egg\n",
            "Copying OpenNMT_py-1.2.0-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding OpenNMT-py 1.2.0 to easy-install.pth file\n",
            "Installing onmt_average_models script to /usr/local/bin\n",
            "Installing onmt_preprocess script to /usr/local/bin\n",
            "Installing onmt_release_model script to /usr/local/bin\n",
            "Installing onmt_server script to /usr/local/bin\n",
            "Installing onmt_train script to /usr/local/bin\n",
            "Installing onmt_translate script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/OpenNMT_py-1.2.0-py3.7.egg\n",
            "Processing dependencies for OpenNMT-py==1.2.0\n",
            "Searching for pyonmttok==1.*\n",
            "Reading https://pypi.org/simple/pyonmttok/\n",
            "Downloading https://files.pythonhosted.org/packages/3a/80/89adcf6836be9f0bb597e2f8fff372eb8c4eef18ee71a5b56c3fd26102da/pyonmttok-1.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl#sha256=20aad32bbb0d8d5cc9565b81e14c49abea9916d901c53211c47de0a1ab877177\n",
            "Best match: pyonmttok 1.29.0\n",
            "Processing pyonmttok-1.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
            "Installing pyonmttok-1.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding pyonmttok 1.29.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/pyonmttok-1.29.0-py3.7-linux-x86_64.egg\n",
            "Searching for waitress\n",
            "Reading https://pypi.org/simple/waitress/\n",
            "Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl#sha256=29af5a53e9fb4e158f525367678b50053808ca6c21ba585754c77d790008c746\n",
            "Best match: waitress 2.0.0\n",
            "Processing waitress-2.0.0-py3-none-any.whl\n",
            "Installing waitress-2.0.0-py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding waitress 2.0.0 to easy-install.pth file\n",
            "Installing waitress-serve script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/waitress-2.0.0-py3.7.egg\n",
            "Searching for configargparse\n",
            "Reading https://pypi.org/simple/configargparse/\n",
            "Downloading https://files.pythonhosted.org/packages/af/cb/2a6620656f029b7b49c302853b433fac2c8eda9cbb5a3bc70b186b1b5b90/ConfigArgParse-1.5.3-py3-none-any.whl#sha256=18f6535a2db9f6e02bd5626cc7455eac3e96b9ab3d969d366f9aafd5c5c00fe7\n",
            "Best match: ConfigArgParse 1.5.3\n",
            "Processing ConfigArgParse-1.5.3-py3-none-any.whl\n",
            "Installing ConfigArgParse-1.5.3-py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding ConfigArgParse 1.5.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/ConfigArgParse-1.5.3-py3.7.egg\n",
            "Searching for torchtext==0.4.0\n",
            "Reading https://pypi.org/simple/torchtext/\n",
            "Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl#sha256=094520d9cd0af6a05368d9023fdc91dc038232bd9d128c7b548ec2200dba53ec\n",
            "Best match: torchtext 0.4.0\n",
            "Processing torchtext-0.4.0-py3-none-any.whl\n",
            "Installing torchtext-0.4.0-py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding torchtext 0.4.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/torchtext-0.4.0-py3.7.egg\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Flask==1.1.4\n",
            "Best match: Flask 1.1.4\n",
            "Adding Flask 1.1.4 to easy-install.pth file\n",
            "Installing flask script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard==2.6.0\n",
            "Best match: tensorboard 2.6.0\n",
            "Adding tensorboard 2.6.0 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for future==0.16.0\n",
            "Best match: future 0.16.0\n",
            "Adding future 0.16.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torch==1.9.0+cu111\n",
            "Best match: torch 1.9.0+cu111\n",
            "Adding torch 1.9.0+cu111 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tqdm==4.62.3\n",
            "Best match: tqdm 4.62.3\n",
            "Adding tqdm 4.62.3 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for click==7.1.2\n",
            "Best match: click 7.1.2\n",
            "Adding click 7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for itsdangerous==1.1.0\n",
            "Best match: itsdangerous 1.1.0\n",
            "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Jinja2==2.11.3\n",
            "Best match: Jinja2 2.11.3\n",
            "Adding Jinja2 2.11.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth==1.35.0\n",
            "Best match: google-auth 1.35.0\n",
            "Adding google-auth 1.35.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Markdown==3.3.4\n",
            "Best match: Markdown 3.3.4\n",
            "Adding Markdown 3.3.4 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for wheel==0.37.0\n",
            "Best match: wheel 0.37.0\n",
            "Adding wheel 0.37.0 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for grpcio==1.41.0\n",
            "Best match: grpcio 1.41.0\n",
            "Adding grpcio 1.41.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard-data-server==0.6.1\n",
            "Best match: tensorboard-data-server 0.6.1\n",
            "Adding tensorboard-data-server 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for absl-py==0.12.0\n",
            "Best match: absl-py 0.12.0\n",
            "Adding absl-py 0.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for protobuf==3.17.3\n",
            "Best match: protobuf 3.17.3\n",
            "Adding protobuf 3.17.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.6\n",
            "Best match: google-auth-oauthlib 0.4.6\n",
            "Adding google-auth-oauthlib 0.4.6 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard-plugin-wit==1.8.0\n",
            "Best match: tensorboard-plugin-wit 1.8.0\n",
            "Adding tensorboard-plugin-wit 1.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==57.4.0\n",
            "Best match: setuptools 57.4.0\n",
            "Adding setuptools 57.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==3.7.4.3\n",
            "Best match: typing-extensions 3.7.4.3\n",
            "Adding typing-extensions 3.7.4.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for MarkupSafe==2.0.1\n",
            "Best match: MarkupSafe 2.0.1\n",
            "Adding MarkupSafe 2.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cachetools==4.2.4\n",
            "Best match: cachetools 4.2.4\n",
            "Adding cachetools 4.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for rsa==4.7.2\n",
            "Best match: rsa 4.7.2\n",
            "Adding rsa 4.7.2 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for importlib-metadata==4.8.1\n",
            "Best match: importlib-metadata 4.8.1\n",
            "Adding importlib-metadata 4.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2021.5.30\n",
            "Best match: certifi 2021.5.30\n",
            "Adding certifi 2021.5.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests-oauthlib==1.3.0\n",
            "Best match: requests-oauthlib 1.3.0\n",
            "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for zipp==3.6.0\n",
            "Best match: zipp 3.6.0\n",
            "Adding zipp 3.6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for oauthlib==3.1.1\n",
            "Best match: oauthlib 3.1.1\n",
            "Adding oauthlib 3.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for OpenNMT-py==1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoKpkQRzsIUr"
      },
      "source": [
        "Убедиться, что установлена правильная версия torchtext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZWzC8Xlk07o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c001de-355d-4014-b62b-8a56ac022529"
      },
      "source": [
        "!pip install torchtext==0.4.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4.0) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHyRn7k_EPC"
      },
      "source": [
        "##**Шаг 2. Загрузка данных**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMNPLAcO_Lw1"
      },
      "source": [
        "Следующий блок скачивает файлы small_vocab_en и small_vocab_ru с https://github.com/susanli2016/NLP-with-Python/tree/master/data и разделяет данные на тренировочные, контрольные (5000 предложений) и тестовые (1000 предложений).\n",
        "\n",
        "* Тренировочные данные (training data) используются для обучения модели.\n",
        "* На контрольных данных (validation data) модель периодически проверяется в процессе обучения, чтобы избежать переобучения.\n",
        "* Тестовые данные (test data) используются для оценки работы модели после завершения обучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgHY_vdv_TFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b8905d-cbf0-4bbc-c913-75108a6483b5"
      },
      "source": [
        "# скачиваем репозиторий с данными\n",
        "!git clone https://github.com/susanli2016/NLP-with-Python.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-with-Python'...\n",
            "remote: Enumerating objects: 324, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 324 (delta 1), reused 0 (delta 0), pack-reused 317\u001b[K\n",
            "Receiving objects: 100% (324/324), 28.18 MiB | 11.17 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXHexUrFQ0G1"
      },
      "source": [
        "Посмотрим число предложений в файлах:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Fqw2WqQ8_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ee179d-7845-4c1c-8f10-b47307c4d46a"
      },
      "source": [
        "# команда cd используется для смены рабочей папки\n",
        "# команда wc в linux выводит на консоль число строк, слов и символов в файлах, указанных после нее\n",
        "! cd NLP-with-Python/data && wc small_vocab_fr small_vocab_en"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  137860  1961295 10135742 small_vocab_fr\n",
            "  137860  1823250  9085267 small_vocab_en\n",
            "  275720  3784545 19221009 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTEpnZGHt-CC"
      },
      "source": [
        "Также посмотрим на их содержимое. Сравните английские и французские предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW02b8cOOQiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee09368-c60c-4006-859d-29529fdc8d02"
      },
      "source": [
        "# команда head в linux позволяет посмотреть первые 10 строк файла\n",
        "# параметр -v добавляет название файла\n",
        "# параметр -n позволяет менять число выводимых строк\n",
        "!head -v -n 10 NLP-with-Python/data/small_vocab_fr\n",
        "!head -v -n 10 NLP-with-Python/data/small_vocab_en      "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> NLP-with-Python/data/small_vocab_fr <==\n",
            "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
            "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
            "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
            "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
            "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
            "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
            "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
            "==> NLP-with-Python/data/small_vocab_en <==\n",
            "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "the united states is usually chilly during july , and it is usually freezing in november .\n",
            "california is usually quiet during march , and it is usually hot in june .\n",
            "the united states is sometimes mild during june , and it is cold in september .\n",
            "your least liked fruit is the grape , but my least liked is the apple .\n",
            "his favorite fruit is the orange , but my favorite is the grape .\n",
            "paris is relaxing during december , but it is usually chilly in july .\n",
            "new jersey is busy during spring , and it is never hot in march .\n",
            "our least liked fruit is the lemon , but my least liked is the grape .\n",
            "the united states is sometimes busy during january , and it is sometimes warm in november .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9JvMgf-QODX"
      },
      "source": [
        "Как мы видим, эти данные уже токенизированы и приведены к нижнему регистру.\n",
        "\n",
        "Теперь разделим их на тренировочные, контрольные и тестовые. Код очень простой -- просмотрите его.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGMDYfRCABby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5538e8-400a-4528-dafe-3893d0311828"
      },
      "source": [
        "# этот блок переменых испольщуется для называния файлов.\n",
        "data = \"NLP-with-Python/data/small_vocab\"\n",
        "path = \"OpenNMT-py/data/\"\n",
        "val = path + \"demo_val\"\n",
        "train = path + \"demo_train\"\n",
        "test = path + \"demo_test\"\n",
        "fr = \"_fr.txt\"\n",
        "en =  \"_en.txt\"\n",
        "\n",
        "def divide(datafile, valfile, trainfile, testfile):\n",
        "    data = open(datafile, encoding = 'UTF-8')\n",
        "    val = open(valfile, 'w', encoding = 'UTF-8')\n",
        "    train = open(trainfile, 'w', encoding = 'UTF-8')\n",
        "    test = open(testfile, 'w', encoding = 'UTF-8')\n",
        "\n",
        "    # обратите внимание, в этой функции для print-а. Посмотрите, как они сработают ниже.\n",
        "    print('Dividing', datafile)\n",
        "    for i, line in enumerate(data):\n",
        "        if i < 1000:\n",
        "            test.write(line)\n",
        "        elif (i >= 1000) and (i<6000) :\n",
        "            val.write(line)\n",
        "        else:\n",
        "            train.write(line)\n",
        "    print(\"Complete\")\n",
        "\n",
        "    data.close()\n",
        "    val.close()\n",
        "    train.close()\n",
        "    test.close()\n",
        "\n",
        "# предыдущая функция вызывается два раза\n",
        "divide(data+\"_fr\", val+fr, train+fr, test+fr)\n",
        "divide(data+\"_en\", val+en, train+en, test+en)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing NLP-with-Python/data/small_vocab_fr\n",
            "Complete\n",
            "Dividing NLP-with-Python/data/small_vocab_en\n",
            "Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dDUt9ygC5w2"
      },
      "source": [
        "##**Шаг 3. Предобработка данных**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO3f7t_QNILI"
      },
      "source": [
        "Следующая команда запускает предварительную обработку данных, в ходе которой будут созданы файлы pyTorch с тренировочными и контрольными данными и словарем. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGbc6GW-NgUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ce1967-fe3b-4060-f009-03e7ef43b6d7"
      },
      "source": [
        "!cd OpenNMT-py && python preprocess.py -train_src data/demo_train_fr.txt -train_tgt data/demo_train_en.txt -valid_src data/demo_val_fr.txt -valid_tgt data/demo_val_en.txt -save_data data/demo"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-05 17:07:46,147 INFO] Extracting features...\n",
            "[2021-11-05 17:07:46,147 INFO]  * number of source features: 0.\n",
            "[2021-11-05 17:07:46,147 INFO]  * number of target features: 0.\n",
            "[2021-11-05 17:07:46,147 INFO] Building `Fields` object...\n",
            "[2021-11-05 17:07:46,147 INFO] Building & saving training data...\n",
            "[2021-11-05 17:07:46,380 INFO] Building shard 0.\n",
            "[2021-11-05 17:07:51,514 INFO]  * saving 0th train data shard to data/demo.train.0.pt.\n",
            "[2021-11-05 17:07:55,522 INFO]  * tgt vocab size: 231.\n",
            "[2021-11-05 17:07:55,523 INFO]  * src vocab size: 357.\n",
            "[2021-11-05 17:07:55,565 INFO] Building & saving validation data...\n",
            "[2021-11-05 17:07:55,595 INFO] Building shard 0.\n",
            "[2021-11-05 17:07:55,678 INFO]  * saving 0th valid data shard to data/demo.valid.0.pt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS9J8VX7w8Fu"
      },
      "source": [
        "Используемые параметры:\n",
        "* -train_src тренировочные данные на входном языке\n",
        "* -train_tgt тренировочные данные на выходном языке\n",
        "* -valid_src контрольные данные на входном языке\n",
        "* -valid_tgt контрольные данные на выходном языке\n",
        "* -save_data путь для сохранения обработанных данных\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOD_8R-uVGhb"
      },
      "source": [
        "## **Шаг 4. Обучение модели**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNo6PeDJV70B"
      },
      "source": [
        "Следующая команда запускает обученние нейронной сети. \n",
        "\n",
        "На экране можно следить за значениями acc и ppl. Первое – точность (accuracy), она будет расти по мере обучения. Второе – перплексия (perplexity), чем она ниже, тем лучше. Также будут выводиться эти значения для контрольных данных.\n",
        "Обучаться будет долго.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6IPp7DKV8Ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3fc9b9-3060-4de9-8e3d-27c113e65c80"
      },
      "source": [
        "!cd OpenNMT-py && python train.py -data data/demo -save_model demo-model -train_steps 3000 -save_checkpoint_steps 500 -valid_steps 500 -world_size 1 -gpu_ranks 0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-05 17:08:12,177 INFO]  * src vocab size = 357\n",
            "[2021-11-05 17:08:12,177 INFO]  * tgt vocab size = 231\n",
            "[2021-11-05 17:08:12,177 INFO] Building model...\n",
            "[2021-11-05 17:08:26,849 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(357, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(231, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1000, 500)\n",
            "        (1): LSTMCell(500, 500)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
            "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=231, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-11-05 17:08:26,850 INFO] encoder: 4186500\n",
            "[2021-11-05 17:08:26,850 INFO] decoder: 5989231\n",
            "[2021-11-05 17:08:26,850 INFO] * number of parameters: 10175731\n",
            "[2021-11-05 17:08:26,852 INFO] Starting training on GPU: [0]\n",
            "[2021-11-05 17:08:26,852 INFO] Start training loop and validate every 500 steps...\n",
            "[2021-11-05 17:08:26,852 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2021-11-05 17:08:30,648 INFO] number of examples: 131860\n",
            "[2021-11-05 17:08:36,011 INFO] Step 50/ 3000; acc:   7.31; ppl: 2090.67; xent: 7.65; lr: 1.00000; 4856/4811 tok/s;      9 sec\n",
            "[2021-11-05 17:08:40,687 INFO] Step 100/ 3000; acc:  13.33; ppl: 205.59; xent: 5.33; lr: 1.00000; 9912/9907 tok/s;     14 sec\n",
            "[2021-11-05 17:08:45,323 INFO] Step 150/ 3000; acc:  30.54; ppl: 25.12; xent: 3.22; lr: 1.00000; 9733/9968 tok/s;     18 sec\n",
            "[2021-11-05 17:08:49,982 INFO] Step 200/ 3000; acc:  42.20; ppl: 11.07; xent: 2.40; lr: 1.00000; 9797/9790 tok/s;     23 sec\n",
            "[2021-11-05 17:08:54,570 INFO] Step 250/ 3000; acc:  54.13; ppl:  6.28; xent: 1.84; lr: 1.00000; 9744/9954 tok/s;     28 sec\n",
            "[2021-11-05 17:08:59,220 INFO] Step 300/ 3000; acc:  59.42; ppl:  4.64; xent: 1.54; lr: 1.00000; 10007/9866 tok/s;     32 sec\n",
            "[2021-11-05 17:09:03,827 INFO] Step 350/ 3000; acc:  60.20; ppl:  4.31; xent: 1.46; lr: 1.00000; 9947/9897 tok/s;     37 sec\n",
            "[2021-11-05 17:09:08,557 INFO] Step 400/ 3000; acc:  63.87; ppl:  3.37; xent: 1.21; lr: 1.00000; 9797/9952 tok/s;     42 sec\n",
            "[2021-11-05 17:09:13,083 INFO] Step 450/ 3000; acc:  62.01; ppl:  3.77; xent: 1.33; lr: 1.00000; 9707/9872 tok/s;     46 sec\n",
            "[2021-11-05 17:09:17,583 INFO] Step 500/ 3000; acc:  62.50; ppl:  3.58; xent: 1.27; lr: 1.00000; 9912/9924 tok/s;     51 sec\n",
            "[2021-11-05 17:09:17,584 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2021-11-05 17:09:17,706 INFO] number of examples: 5000\n",
            "[2021-11-05 17:09:21,635 INFO] Validation perplexity: 3.19318\n",
            "[2021-11-05 17:09:21,635 INFO] Validation accuracy: 64.337\n",
            "[2021-11-05 17:09:21,639 INFO] Saving checkpoint demo-model_step_500.pt\n",
            "[2021-11-05 17:09:26,503 INFO] Step 550/ 3000; acc:  64.96; ppl:  3.00; xent: 1.10; lr: 1.00000; 5335/5271 tok/s;     60 sec\n",
            "[2021-11-05 17:09:31,315 INFO] Step 600/ 3000; acc:  63.86; ppl:  3.23; xent: 1.17; lr: 1.00000; 9736/9892 tok/s;     64 sec\n",
            "[2021-11-05 17:09:36,065 INFO] Step 650/ 3000; acc:  68.43; ppl:  2.64; xent: 0.97; lr: 1.00000; 9944/9877 tok/s;     69 sec\n",
            "[2021-11-05 17:09:40,409 INFO] Step 700/ 3000; acc:  65.61; ppl:  3.19; xent: 1.16; lr: 1.00000; 9896/9704 tok/s;     74 sec\n",
            "[2021-11-05 17:09:44,851 INFO] Step 750/ 3000; acc:  66.87; ppl:  2.86; xent: 1.05; lr: 1.00000; 9799/9856 tok/s;     78 sec\n",
            "[2021-11-05 17:09:49,584 INFO] Step 800/ 3000; acc:  69.32; ppl:  2.59; xent: 0.95; lr: 1.00000; 9804/9879 tok/s;     83 sec\n",
            "[2021-11-05 17:09:54,143 INFO] Step 850/ 3000; acc:  70.27; ppl:  2.56; xent: 0.94; lr: 1.00000; 9749/9820 tok/s;     87 sec\n",
            "[2021-11-05 17:09:58,941 INFO] Step 900/ 3000; acc:  71.61; ppl:  2.60; xent: 0.95; lr: 1.00000; 9876/9872 tok/s;     92 sec\n",
            "[2021-11-05 17:10:03,694 INFO] Step 950/ 3000; acc:  74.94; ppl:  2.24; xent: 0.81; lr: 1.00000; 10006/9820 tok/s;     97 sec\n",
            "[2021-11-05 17:10:08,353 INFO] Step 1000/ 3000; acc:  76.97; ppl:  2.08; xent: 0.73; lr: 1.00000; 9871/9694 tok/s;    102 sec\n",
            "[2021-11-05 17:10:08,353 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2021-11-05 17:10:08,478 INFO] number of examples: 5000\n",
            "[2021-11-05 17:10:12,450 INFO] Validation perplexity: 2.06548\n",
            "[2021-11-05 17:10:12,450 INFO] Validation accuracy: 77.7612\n",
            "[2021-11-05 17:10:12,453 INFO] Saving checkpoint demo-model_step_1000.pt\n",
            "[2021-11-05 17:10:17,172 INFO] Step 1050/ 3000; acc:  80.44; ppl:  1.87; xent: 0.62; lr: 1.00000; 5195/5172 tok/s;    110 sec\n",
            "[2021-11-05 17:10:21,640 INFO] Step 1100/ 3000; acc:  85.55; ppl:  1.63; xent: 0.49; lr: 1.00000; 10027/9753 tok/s;    115 sec\n",
            "[2021-11-05 17:10:26,041 INFO] Step 1150/ 3000; acc:  89.19; ppl:  1.40; xent: 0.33; lr: 1.00000; 9860/9813 tok/s;    119 sec\n",
            "[2021-11-05 17:10:30,719 INFO] Step 1200/ 3000; acc:  92.63; ppl:  1.30; xent: 0.26; lr: 1.00000; 9840/9823 tok/s;    124 sec\n",
            "[2021-11-05 17:10:35,486 INFO] Step 1250/ 3000; acc:  94.13; ppl:  1.20; xent: 0.19; lr: 1.00000; 9788/9901 tok/s;    129 sec\n",
            "[2021-11-05 17:10:39,869 INFO] Step 1300/ 3000; acc:  92.92; ppl:  1.25; xent: 0.22; lr: 1.00000; 9805/9824 tok/s;    133 sec\n",
            "[2021-11-05 17:10:44,566 INFO] Step 1350/ 3000; acc:  95.51; ppl:  1.14; xent: 0.13; lr: 1.00000; 9947/9863 tok/s;    138 sec\n",
            "[2021-11-05 17:10:49,315 INFO] Step 1400/ 3000; acc:  96.00; ppl:  1.13; xent: 0.12; lr: 1.00000; 9833/9776 tok/s;    142 sec\n",
            "[2021-11-05 17:10:54,007 INFO] Step 1450/ 3000; acc:  95.70; ppl:  1.13; xent: 0.12; lr: 1.00000; 9781/9907 tok/s;    147 sec\n",
            "[2021-11-05 17:10:58,663 INFO] Step 1500/ 3000; acc:  95.39; ppl:  1.14; xent: 0.13; lr: 1.00000; 9677/9848 tok/s;    152 sec\n",
            "[2021-11-05 17:10:58,664 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2021-11-05 17:10:58,782 INFO] number of examples: 5000\n",
            "[2021-11-05 17:11:02,765 INFO] Validation perplexity: 1.11052\n",
            "[2021-11-05 17:11:02,765 INFO] Validation accuracy: 96.1358\n",
            "[2021-11-05 17:11:02,768 INFO] Saving checkpoint demo-model_step_1500.pt\n",
            "[2021-11-05 17:11:07,564 INFO] Step 1550/ 3000; acc:  95.59; ppl:  1.13; xent: 0.12; lr: 1.00000; 5264/5206 tok/s;    161 sec\n",
            "[2021-11-05 17:11:12,239 INFO] Step 1600/ 3000; acc:  95.94; ppl:  1.11; xent: 0.10; lr: 1.00000; 9815/9837 tok/s;    165 sec\n",
            "[2021-11-05 17:11:16,878 INFO] Step 1650/ 3000; acc:  95.93; ppl:  1.13; xent: 0.12; lr: 1.00000; 9962/9800 tok/s;    170 sec\n",
            "[2021-11-05 17:11:21,363 INFO] Step 1700/ 3000; acc:  96.04; ppl:  1.10; xent: 0.10; lr: 1.00000; 9861/9765 tok/s;    175 sec\n",
            "[2021-11-05 17:11:25,815 INFO] Step 1750/ 3000; acc:  95.74; ppl:  1.12; xent: 0.12; lr: 1.00000; 9618/9841 tok/s;    179 sec\n",
            "[2021-11-05 17:11:30,446 INFO] Step 1800/ 3000; acc:  96.72; ppl:  1.09; xent: 0.08; lr: 1.00000; 9820/9849 tok/s;    184 sec\n",
            "[2021-11-05 17:11:35,192 INFO] Step 1850/ 3000; acc:  96.87; ppl:  1.07; xent: 0.07; lr: 1.00000; 9872/9898 tok/s;    188 sec\n",
            "[2021-11-05 17:11:39,775 INFO] Step 1900/ 3000; acc:  96.53; ppl:  1.08; xent: 0.08; lr: 1.00000; 9816/9833 tok/s;    193 sec\n",
            "[2021-11-05 17:11:44,423 INFO] Step 1950/ 3000; acc:  97.13; ppl:  1.06; xent: 0.06; lr: 1.00000; 9722/9874 tok/s;    198 sec\n",
            "[2021-11-05 17:11:48,972 INFO] Step 2000/ 3000; acc:  96.68; ppl:  1.09; xent: 0.08; lr: 1.00000; 9892/9828 tok/s;    202 sec\n",
            "[2021-11-05 17:11:48,972 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2021-11-05 17:11:49,088 INFO] number of examples: 5000\n",
            "[2021-11-05 17:11:53,057 INFO] Validation perplexity: 1.07214\n",
            "[2021-11-05 17:11:53,058 INFO] Validation accuracy: 96.8656\n",
            "[2021-11-05 17:11:53,061 INFO] Saving checkpoint demo-model_step_2000.pt\n",
            "[2021-11-05 17:11:57,758 INFO] Step 2050/ 3000; acc:  97.30; ppl:  1.06; xent: 0.06; lr: 1.00000; 5193/5159 tok/s;    211 sec\n",
            "[2021-11-05 17:11:58,676 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2021-11-05 17:12:03,290 INFO] number of examples: 131860\n",
            "[2021-11-05 17:12:07,618 INFO] Step 2100/ 3000; acc:  95.96; ppl:  1.10; xent: 0.10; lr: 1.00000; 4375/4307 tok/s;    221 sec\n",
            "[2021-11-05 17:12:12,333 INFO] Step 2150/ 3000; acc:  97.29; ppl:  1.06; xent: 0.06; lr: 1.00000; 9802/9825 tok/s;    225 sec\n",
            "[2021-11-05 17:12:17,079 INFO] Step 2200/ 3000; acc:  97.16; ppl:  1.06; xent: 0.06; lr: 1.00000; 9831/9952 tok/s;    230 sec\n",
            "[2021-11-05 17:12:21,571 INFO] Step 2250/ 3000; acc:  96.95; ppl:  1.07; xent: 0.07; lr: 1.00000; 9818/9826 tok/s;    235 sec\n",
            "[2021-11-05 17:12:26,190 INFO] Step 2300/ 3000; acc:  96.76; ppl:  1.09; xent: 0.08; lr: 1.00000; 9599/9862 tok/s;    239 sec\n",
            "[2021-11-05 17:12:30,988 INFO] Step 2350/ 3000; acc:  97.14; ppl:  1.07; xent: 0.07; lr: 1.00000; 9963/9851 tok/s;    244 sec\n",
            "[2021-11-05 17:12:35,489 INFO] Step 2400/ 3000; acc:  96.72; ppl:  1.08; xent: 0.08; lr: 1.00000; 9870/9835 tok/s;    249 sec\n",
            "[2021-11-05 17:12:40,359 INFO] Step 2450/ 3000; acc:  97.61; ppl:  1.05; xent: 0.05; lr: 1.00000; 9724/9848 tok/s;    254 sec\n",
            "[2021-11-05 17:12:44,922 INFO] Step 2500/ 3000; acc:  96.99; ppl:  1.08; xent: 0.07; lr: 1.00000; 9713/9888 tok/s;    258 sec\n",
            "[2021-11-05 17:12:44,922 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2021-11-05 17:12:45,033 INFO] number of examples: 5000\n",
            "[2021-11-05 17:12:49,012 INFO] Validation perplexity: 1.06602\n",
            "[2021-11-05 17:12:49,013 INFO] Validation accuracy: 97.2892\n",
            "[2021-11-05 17:12:49,015 INFO] Saving checkpoint demo-model_step_2500.pt\n",
            "[2021-11-05 17:12:53,578 INFO] Step 2550/ 3000; acc:  96.67; ppl:  1.08; xent: 0.08; lr: 1.00000; 5080/5095 tok/s;    267 sec\n",
            "[2021-11-05 17:12:58,362 INFO] Step 2600/ 3000; acc:  97.20; ppl:  1.06; xent: 0.06; lr: 1.00000; 9926/9886 tok/s;    272 sec\n",
            "[2021-11-05 17:13:03,041 INFO] Step 2650/ 3000; acc:  96.42; ppl:  1.09; xent: 0.08; lr: 1.00000; 9734/9773 tok/s;    276 sec\n",
            "[2021-11-05 17:13:07,876 INFO] Step 2700/ 3000; acc:  97.43; ppl:  1.05; xent: 0.05; lr: 1.00000; 9928/9848 tok/s;    281 sec\n",
            "[2021-11-05 17:13:12,379 INFO] Step 2750/ 3000; acc:  96.87; ppl:  1.09; xent: 0.08; lr: 1.00000; 9858/9776 tok/s;    286 sec\n",
            "[2021-11-05 17:13:16,829 INFO] Step 2800/ 3000; acc:  97.51; ppl:  1.06; xent: 0.05; lr: 1.00000; 9767/9849 tok/s;    290 sec\n",
            "[2021-11-05 17:13:21,438 INFO] Step 2850/ 3000; acc:  97.62; ppl:  1.05; xent: 0.05; lr: 1.00000; 9861/9847 tok/s;    295 sec\n",
            "[2021-11-05 17:13:25,980 INFO] Step 2900/ 3000; acc:  97.44; ppl:  1.05; xent: 0.05; lr: 1.00000; 9728/9821 tok/s;    299 sec\n",
            "[2021-11-05 17:13:30,781 INFO] Step 2950/ 3000; acc:  97.29; ppl:  1.06; xent: 0.05; lr: 1.00000; 9845/9868 tok/s;    304 sec\n",
            "[2021-11-05 17:13:35,558 INFO] Step 3000/ 3000; acc:  97.67; ppl:  1.04; xent: 0.04; lr: 1.00000; 9994/9820 tok/s;    309 sec\n",
            "[2021-11-05 17:13:35,558 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2021-11-05 17:13:35,666 INFO] number of examples: 5000\n",
            "[2021-11-05 17:13:39,630 INFO] Validation perplexity: 1.0605\n",
            "[2021-11-05 17:13:39,630 INFO] Validation accuracy: 97.3437\n",
            "[2021-11-05 17:13:39,633 INFO] Saving checkpoint demo-model_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK59kcFxxLKN"
      },
      "source": [
        "Используемые параметры:\n",
        "* -data путь к обработанным данным\n",
        "* -save_model путь для сохранения модели\n",
        "* -train_steps время обучения – число шагов\n",
        "* -save_checkpoint_steps как часто сохранять модель – число шагов\n",
        "* -valid_steps как часто оценивать модель – число шагов\n",
        "* -world_size число GPU\n",
        "* -gpu_ranks ранги используемых GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke5ujxfP7bN-"
      },
      "source": [
        "По умолчанию используется модель с двумя слоями LSTM по 500 нейронов для декодера и энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mexDf9EPoCGm"
      },
      "source": [
        "## Шаг 5. Перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdlBZ37YoINx"
      },
      "source": [
        "С помощью следующей команды можно перевести любой файл. Мы переведем тестовые данные, которые создали в начале."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obZFNx7voMt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417f19d5-4dbb-4422-ba44-4c2f67867e2f"
      },
      "source": [
        "#!cd OpenNMT-py && python translate.py -model demo-model_step_5000.pt -src data/demo_test_fr.txt -output pred.txt -replace_unk\n",
        "!cd OpenNMT-py && python translate.py -model demo-model_step_3000.pt -src data/demo_test_fr.txt -output pred.txt -replace_unk"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-05 17:14:10,950 INFO] Translating shard 0.\n",
            "/content/OpenNMT-py/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "/content/OpenNMT-py/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [10, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
            "[2021-11-05 17:14:31,165 INFO] PRED AVG SCORE: -0.0245, PRED PPL: 1.0248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UcYuWz9xnC1"
      },
      "source": [
        "Используемые параметры: \n",
        "* -model путь к файлу модели\n",
        "* -src путь к файлу с тестовыми данными на входном языке\n",
        "* -output путь для сохранения переводов модели\n",
        "* -replace_unk  заменить токены UNK на токены входного языка с максимальным весом внимания\n",
        "\n",
        "Также можно использовать:\n",
        "* -verbose выводить на консоль переводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FeGtrh8oxEA"
      },
      "source": [
        "Посмотрим на переводы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIqRZbAAovwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bcb891-461d-44ee-ba5c-51bd16c04745"
      },
      "source": [
        "print('Оригинал:')\n",
        "!head -n 15 OpenNMT-py/data/demo_test_fr.txt\n",
        "print('-' * 50) #разделитель\n",
        "print('Перевод модели:')\n",
        "!head -n 15 OpenNMT-py/pred.txt\n",
        "print('-' * 50) #разделитель\n",
        "print('Эталонный перевод:')\n",
        "!head  -n 15 OpenNMT-py/data/demo_test_en.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оригинал:\n",
            "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
            "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
            "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
            "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
            "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
            "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
            "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
            "la chaux est son moins aimé des fruits , mais la banane est mon moins aimé.\n",
            "il a vu un vieux camion jaune .\n",
            "inde est pluvieux en juin , et il est parfois chaud en novembre .\n",
            "ce chat était mon animal le plus aimé .\n",
            "il n'aime pamplemousse , citrons verts et les citrons .\n",
            "--------------------------------------------------\n",
            "Перевод модели:\n",
            "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "the united states is usually chilly during july , and it is usually freezing in november .\n",
            "california is usually quiet during march , and it is usually hot in june .\n",
            "the united states is sometimes mild during june , and it is cold in september .\n",
            "your least liked fruit is the grape , but my least liked is the apple.\n",
            "his favorite fruit is the orange , but my favorite is the grape .\n",
            "paris is relaxing during december , but it is usually chilly in july .\n",
            "new jersey is busy during spring , and it is never hot in march .\n",
            "our least liked fruit is the lemon , but my least liked is the grape .\n",
            "the united states is sometimes busy during january , and it is sometimes hot in november .\n",
            "the lime is his least liked fruit , but the banana is my least liked .\n",
            "he saw a old yellow truck .\n",
            "india is rainy during june , and it is sometimes hot in november .\n",
            "that cat was my most loved animal .\n",
            "he dislikes grapefruit , limes , and lemons .\n",
            "--------------------------------------------------\n",
            "Эталонный перевод:\n",
            "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "the united states is usually chilly during july , and it is usually freezing in november .\n",
            "california is usually quiet during march , and it is usually hot in june .\n",
            "the united states is sometimes mild during june , and it is cold in september .\n",
            "your least liked fruit is the grape , but my least liked is the apple .\n",
            "his favorite fruit is the orange , but my favorite is the grape .\n",
            "paris is relaxing during december , but it is usually chilly in july .\n",
            "new jersey is busy during spring , and it is never hot in march .\n",
            "our least liked fruit is the lemon , but my least liked is the grape .\n",
            "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
            "the lime is her least liked fruit , but the banana is my least liked .\n",
            "he saw a old yellow truck .\n",
            "india is rainy during june , and it is sometimes warm in november .\n",
            "that cat was my most loved animal .\n",
            "he dislikes grapefruit , limes , and lemons .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQdr1jO_DMV6"
      },
      "source": [
        "# **ЗАДАЧКА!!!** Поменяйте, пожалуйста, код в предыдущем блоке так, чтобы видеть не 5 предложений, а 15."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBFcCQ2PqXP6"
      },
      "source": [
        "## Шаг 6. Оценка модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naa26PLpqc2P"
      },
      "source": [
        "**BLEU** – автоматическая метрика оценки качества перевода, которая использует идею «чем ближе машинный перевод к переводу профессионального переводчика, тем он лучше». Алгоритм сравнивает n-граммы из перевода-кандидата с эталонным переводом, также производится подсчет совпадений.  Чем больше число совпадений, тем лучше качество перевода-кандидата. BLEU является, пожалуй, самой используемой метрикой оценки машинного перевода. Именно ее значения можно увидеть в научных статьях и таблицах результатов соревнований по машинному переводу. Например, лучший результат соревнований на тестовых данных WMT19 в русско-английском переводе составил 40.2, а в англо-русском – 36.3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXL1YKuVqlyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701ce1e1-47f9-424f-9a3d-5fffd040b32c"
      },
      "source": [
        "!cd OpenNMT-py && perl tools/multi-bleu.perl data/demo_test_en.txt < pred.txt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU = 93.91, 97.4/95.0/92.8/90.5 (BP=1.000, ratio=1.000, hyp_len=13209, ref_len=13205)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djJT0_LWrEL2"
      },
      "source": [
        "Так как наш корпус имеет ограниченный словарь и тест проводился на части корпуса, BLEU окажется нереально высоким по сравнению с результатами соревнований. Если провести тест на независимых данных, результат будет совсем иной – большинство слов попросту не будут известны нашей модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdGK2Td_E2DG"
      },
      "source": [
        "# Шаг 7. Немножко теории, для просмотра -- выполнять не требуется"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A2YtxGZEtLf"
      },
      "source": [
        "# Другие модели\n",
        "Ниже список параметров, которые можно использовать для изменения архитектуры модели. В квадратных скобах перечислены возможные значения праметров.\n",
        "\n",
        "\n",
        "-model_type [text|img|audio|vec] тип модели\n",
        "\n",
        "-encoder_type [rnn|brnn|ggnn|mean|transformer|cnn] тип энкодера\n",
        "\n",
        "-decoder_type [rnn|transformer|cnn] тип декодера\n",
        "\n",
        "-layers число слоев в энкодере и декодере (или по отдельности -enc_layers и -dec_layers)\n",
        "\n",
        "-rnn_size число скрытых состояний RNN (или по отдельности -enc_rnn_size и -dec_rnn_size)\n",
        "\n",
        "-rnn_type [LSTM, GRU, SRU] тип RNN\n",
        "\n",
        "\n",
        "Больше разных параметров обучения можно найти в официальной документации https://opennmt.net/OpenNMT-py/options/train.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eedyKFjLFjmu"
      },
      "source": [
        "# А как же трансформер?\n",
        "\n",
        "Возможно, вы уже слышали, что одной из самых передовых архитектур для машинного перевода сейчас является Трансформер, предложенный в научной статье Attention is all you need. Как можно его использовать?\n",
        "\n",
        "Трансформер требует использования графических процессоров (GPU). Если у вас есть компьютер хотя бы с одной мощной видеокартой, можно попробовать запустить Трансформер с помощью вот такой команды.\n",
        "\n",
        "python train.py -data data/demo -save_model model_demo -layers 6 -rnn_size 512 \\\n",
        "-word_vec_size 512 -transformer_ff 2048 -heads 8 -encoder_type transformer \\\n",
        "-decoder_type transformer -position_encoding -train_steps 80000 \\\n",
        " -max_generator_batches 2 -dropout 0.1 -batch_size 2048 -batch_type tokens \\\n"
      ]
    }
  ]
}